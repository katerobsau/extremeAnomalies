---
title: "Vignette"
format: html
editor: visual
bibliography: references.bib
---

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(here)
library(rnoaa)

source("../R/get_data_for_fig.R")
source("../R/create_spatial_plot.R")
source("../R/create_temporal_plot.R")

meta_data <- readRDS("../data/aus_stations_meta.rds")
```

## Introduction

Para 1 - Automated testing is important for large scale data handling of observations and analysis of extreme events.

Para 2 - There are a suite of tests of available for quality assurance.

Para 3 - These do not necessarily guarantee the quality of the extreme observations. Known issues persist in this data that impact the analysis of extremes.

Para 4 - Here we provide methods and examples showing how to better assess the quality of the extreme rainfall observations.

## Data

Used the Global Historical Climate Network of Daily data (GHCN Daily).

These are the important variables used:

-   PRCP = Precipitation (tenths of mm)
-   MDPR = Multiday precipitation total (tenths of mm; use with DAPR and DWPR, if available)
-   DAPR = Number of days included in the multiday precipiation total (MDPR)
-   DWPR = Number of days with non-zero precipitation included in multiday precipitation total (MDPR)

These variables are as described in the [GHCN daily readme](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt).

There are fourteen flags associated with automatic testing[@durre2010].

| Quality Assurance Test       | Details                                                                        | Flag |
|-----------------|--------------------------------------|-----------------|
| NA                           | Did not fail any quality assurance check                                       | ""   |
| Duplicate check              | Identifies duplicated observations                                             | "D"  |
| Gap check                    | Identifies maximal differences between observations                            | "G"  |
| Internal Consistency Check   | Ensures data doesn't violate meteorological conditions (e.g negative rainfall) | "I"  |
|                              | Failed streak/frequent-value check                                             | "K"  |
|                              | Failed check on length of multiday period                                      | "L"  |
| Megaconsistency check        | Checks it doesn't break known records                                          | "M"  |
|                              | Failed naught check                                                            | "N"  |
| Climatological outlier check | Checks for abnormally large observations                                       | "O"  |
|                              | Failed lagged range check                                                      | "R"  |
|                              | Failed spatial consistency check                                               | "S"  |
|                              | Failed temporal consistency check                                              | "T"  |
|                              | Temperature too warm for snow                                                  | "W"  |
|                              | Failed bounds check                                                            | "X"  |
|                              | Flagged as a result of an official Datzilla investigation                      | "Z"  |

: 14 Existing Quality Flags.

GHCN Daily data for precipitation can be accessed using the `rnoaa` package [@rnoaa].

### Visualisation

```{r eval = TRUE, echo = FALSE}

fig_data_for_plot <- readRDS("../data/example_1974.rds")

fct_shapes = c(1, 16, 4, 13)
extreme_event_temporal_plot <- ggplot(fig_data_for_plot) +
  geom_point(aes(x = date, y = id,
                 col = prcp_combined, shape = prcp_type),
             size = 4, stroke = 1.15) +
  scale_shape_manual(name = "Obs Type",
                     values = fct_shapes) +
  scale_color_gradient(name = "Prcp (mm)", low = "skyblue", high = "navy") +
  xlab("Date") +
  ylab("Station Id") +
  ggtitle("Rainfall Observations", "Brisbane, Australia January 1974") +
  theme_bw()

extreme_event_temporal_plot
```

Caption: Daily rainfall observations in Brisbane, Australia. Solid circles are coloured by the measured precipitation. Empty circles show zero observations.A grey cross indicates missing data. Tagged accumulations, rainfall totals accumulated over multiple days, are shown with an empty circle and a cross. The final day in the accumulation period is coloured by the multiday total and the other days are coloured grey.

### Quality Issues

On 26-01-1974 there was an extreme rainfall event in Brisbane, Australia, Figure 1. The majority of stations in this region recorded their annual maximum on this date. However, visual inspection of the data reveals numerous data quality errors.

Of the 27 stations in this example on 26-01-1974 there are 6 stations with extreme observations that are not recorded reliably or are erroreous. It appears that 3 maxima occurred within tagged accumulations, 1 station recorded missing data instead of a maximum and 2 observations were falsely recorded as zero instead of tagged accumulations.

If the extremal observations are not treated appropriately during pre-processing, these observations will result in errors in the annual maximum observations. Whilst in general statistical methods will be robust to some errors within the data, care should be taken to identify as many systematic and prevalent sources of error as possible. Particularly, for significant extreme events causing impacts or for record breaking observations.

## Quality Control Issues

### True extremes values are not outliers

**Problem**

The current outlier test used to automtically flag outliers assumes normality. This is a poor assumption for rainfall. Rainfall distributions are skew and are commonly heavy tailed. True extreme observations are therefore being incorrectly flagged as outliers.

Given the first step in an analysis is commonly to filter the data to remove observations flagged for poor quality this is concern. People are removing true extreme values from their analysis. The values that are being removed are often the largest observations on the record. Such observations are known to have a large impact estimates of high quanitles from the fitted distribution[@miralles2023].

**Examples in data**

```{r outliers, echo = FALSE}
outlier_data <- readRDS("../data/outliers.rds")
outlier_reflag_data <- readRDS("../data/prcp_reflag.rds")
num_outliers = nrow(outlier_data)
num_outliers_w_nbrs = nrow(outlier_reflag_data)
reflag_count = outlier_reflag_data$new_flag |> table()

```

Using a subset of stations from Southeast Queensland in Australia, we find there are 62 observations flagged as outliers from 1297 stations (\~5%). For 60 of these stations, there are sufficient neighbours to analysis the spatial consistency of these "so-called" outliers.

We find that 37 of 60 of these observations would pass a spatial consistency check and are spuriously flagged. This is a false-positive rate of 62% and provides a clear indication that this flag is wholly unsuitable. The false-positive rate may in fact be higher given we also suspect that the spatial consistency check is not particularly well formulated for rainfall *(need to provide more detail here)*.\
\
\* Should go through and visually inspect all 60\*

Example 1: Correct outlier flag.

```{r eg1, cache = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
extreme_date = as_date("1893-06-09")
stn_id = "ASN00040059"
stn_meta = meta_data |> 
  filter(id == stn_id)

fig_data_for_plot <- get_data_for_fig(
  extreme_date, 
  stn_lat = stn_meta$latitude, 
  stn_long = stn_meta$longitude, 
  meta_data = meta_data)

obs_month_str = month(extreme_date, label = TRUE, abbr = TRUE) |>
  as.character()

eg1_outlier_plot <- create_temporal_plot(
  fig_data_for_plot,
  city_country_str = stn_meta$name,
  month_year_str = paste(obs_month_str, year(extreme_date)))

eg1_outlier_plot
```

Example 2: Incorrect outlier flag (spatial consistent observations).

```{r eg2, cache = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
extreme_date = as_date("1893-06-09")
stn_id = "ASN00040059"
stn_meta = meta_data |> 
  filter(id == stn_id)

fig_data_for_plot <- get_data_for_fig(
  extreme_date, 
  stn_lat = stn_meta$latitude, 
  stn_long = stn_meta$longitude, 
  meta_data = meta_data)

obs_month_str = month(extreme_date, label = TRUE, abbr = TRUE) |>
  as.character()

eg2_outlier_plot <- create_temporal_plot(
  fig_data_for_plot,
  city_country_str = stn_meta$name,
  month_year_str = paste(obs_month_str, year(extreme_date)))

eg2_outlier_plot
```

Example 3: Incorrect outlier flag, but flagged spatial inconsistent.

```{r eg3, cache = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
extreme_date = as_date("1893-06-09")
stn_id = "ASN00040059"
stn_meta = meta_data |> 
  filter(id == stn_id)

fig_data_for_plot <- get_data_for_fig(
  extreme_date, 
  stn_lat = stn_meta$latitude, 
  stn_long = stn_meta$longitude, 
  meta_data = meta_data)

obs_month_str = month(extreme_date, label = TRUE, abbr = TRUE) |>
  as.character()

eg2_outlier_plot <- create_temporal_plot(
  fig_data_for_plot,
  city_country_str = stn_meta$name,
  month_year_str = paste(obs_month_str, year(extreme_date)))

eg2_outlier_plot
```

### Suggested Approach

The automated quality testing runs the test for outlier first. Flagging any observations that fail the test. Once an observation is flagged it is not used in any further quality testing. It is clear, that where possible a spatial consistency check should also be used to consider the suitability of these observations for use in analysis.

Arguably, the autmatic outlier flag should be ignored without further investigation. At a minimum exploratory data visualisation for any record breaking observations.

-   meta data or historical newspapers
-   Need a better solution.

### Fit a GEV with and without obs to show impact

-   Miralles and Davision trigger event paper
-   Venezuala example (classic)
-   Don't have other examples in the data
-   How many "O"'s aren't really "O"'s - false positive rate
-   Demonstrate this in the data somehow

### Untagged Accumulations

Example 4: Sunday-Monday Untagged Accumulations
```{r eg4, cache = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
grouped <- readRDS("../data/sunmon_acc.rds")

grouped %>%
  ggplot() +
  geom_col(aes(x = day, y = raindays)) +
  facet_wrap(~ year) +
  theme_bw() +
  labs(x = "Day of the Week",
       y = "Count of Rain Days")
```



### Flagging systematic missingness

### Conclusions

Write some stuff here

### Appendix

Other fun quality issues:

-   Elevation

-   Records prior to European arrival in Australia

Curious but aside - How does this carry through to the gridded data.

### Example Issue Write Up

-   Explain the problem
-   Identify and show examples of this quality control issue in the data
-   Provide R function(s) to address the issue
-   Provide suggested updated quality flag
-   Depending on the issue, demonstrate the impact of this on a basic GEV analysis with and without the fix.
