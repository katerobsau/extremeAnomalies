---
title: "Vignette"
format: html
editor: visual
bibliography: references.bib
---

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(here)
library(rnoaa)

source("../R/get_data_for_fig.R")

meta_data <- readRDS("../data/aus_stations_meta.rds")
```

## Introduction

Para 1 - Automated testing is important for large scale data handling of observations and analysis of extreme events.

Para 2 - There are a suite of tests of available for quality assurance.

Para 3 - These do not necessarily guarantee the quality of the extreme observations. Known issues persist in this data that impact the analysis of extremes.

Para 4 - Here we provide methods and examples showing how to better assess the quality of the extreme rainfall observations.

## Data

Used the Global Historical Climate Network of Daily data (GHCN Daily).

These are the important variables used:

-   PRCP = Precipitation (tenths of mm)
-   MDPR = Multiday precipitation total (tenths of mm; use with DAPR and DWPR, if available)
-   DAPR = Number of days included in the multiday precipiation total (MDPR)
-   DWPR = Number of days with non-zero precipitation included in multiday precipitation total (MDPR)

These variables are as described in the [GHCN daily readme](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt).

There are fourteen flags associated with automatic testing[@durre2010].

| Quality Assurance Test       | Details                                                                        | Flag |
|-----------------|--------------------------------------|-----------------|
| NA                           | Did not fail any quality assurance check                                       | ""   |
| Duplicate check              | Identifies duplicated observations                                             | "D"  |
| Gap check                    | Identifies maximal differences between observations                            | "G"  |
| Internal Consistency Check   | Ensures data doesn't violate meteorological conditions (e.g negative rainfall) | "I"  |
|                              | Failed streak/frequent-value check                                             | "K"  |
|                              | Failed check on length of multiday period                                      | "L"  |
| Megaconsistency check        | Checks it doesn't break known records                                          | "M"  |
|                              | Failed naught check                                                            | "N"  |
| Climatological outlier check | Checks for abnormally large observations                                       | "O"  |
|                              | Failed lagged range check                                                      | "R"  |
|                              | Failed spatial consistency check                                               | "S"  |
|                              | Failed temporal consistency check                                              | "T"  |
|                              | Temperature too warm for snow                                                  | "W"  |
|                              | Failed bounds check                                                            | "X"  |
|                              | Flagged as a result of an official Datzilla investigation                      | "Z"  |

: 14 Existing Quality Flags.

GHCN Daily data for precipitation can be accessed using the `rnoaa` package [@rnoaa].

### Visualisation

```{r eval = TRUE, echo = FALSE}

fig_data_for_plot <- readRDS("../data/example_1974.rds")

fct_shapes = c(1, 16, 4, 13)
extreme_event_temporal_plot <- ggplot(fig_data_for_plot) +
  geom_point(aes(x = date, y = id,
                 col = prcp_combined, shape = prcp_type),
             size = 4, stroke = 1.15) +
  scale_shape_manual(name = "Obs Type",
                     values = fct_shapes) +
  scale_color_gradient(name = "Prcp (mm)", low = "skyblue", high = "navy") +
  xlab("Date") +
  ylab("Station Id") +
  ggtitle("Rainfall Observations", "Brisbane, Australia January 1974") +
  theme_bw()

extreme_event_temporal_plot
```

Caption: Daily rainfall observations in Brisbane, Australia. Solid circles are coloured by the measured precipitation. Empty circles show zero observations.A grey cross indicates missing data. Tagged accumulations, rainfall totals accumulated over multiple days, are shown with an empty circle and a cross. The final day in the accumulation period is coloured by the multiday total and the other days are coloured grey.

### Quality Issues

On 26-01-1974 there was an extreme rainfall event in Brisbane, Australia, Figure 1. The majority of stations in this region recorded their annual maximum on this date. However, visual inspection of the data reveals numerous data quality errors.

Of the 27 stations in this example on 26-01-1974 there are 6 stations with extreme observations that are not recorded reliably or are erroreous. It appears that 3 maxima occurred within tagged accumulations, 1 station recorded missing data instead of a maximum and 2 observations were falsely recorded as zero instead of tagged accumulations.

If the extremal observations are not treated appropriately during pre-processing, these observations will result in errors in the annual maximum observations. Whilst in general statistical methods will be robust to some errors within the data, care should be taken to identify as many systematic and prevalent sources of error as possible. Particularly, for significant extreme events causing impacts or for record breaking observations.

## Quality Control Issues

### Outliers vs Extremes

**Problem**

The current outlier test used to automtically flag outliers assumes normality. This is a poor assumption for rainfall. Rainfall distributions are skew and the tail of the distribution is commonly heavy. This leds to false outlier flags of extreme observations.

The first step in an analysis is commonly to filter the data to remove observations that have been flagged for poor quality. This means people are removing true extreme values in their analysis.

**Example**

```{r}
outlier_data <- readRDS("../data/outliers.rds")
outlier_reflag_data <- readRDS("../data/prcp_reflag.rds")
num_outliers = nrow(outlier_data)
num_outliers_w_nbrs = nrow(outlier_reflag_data)
reflag_count = outlier_reflag_data$new_flag |> table()

```

Using a subset of stations from Southeast Queensland in Australia, we find 62 observations flagged as outliers from 1297 stations (\~5%).

The automated quality testing runs the test for outlier first. Flagging any observations that fail the test, and not using these in any further quality testing.

For 60 of these stations, there are sufficient neighbours to analysis the spatial consistency of these "so-called" outliers. We find that 37 of 60 of these observations would pass a spatial consistency check and are not outliers. This is a false-positive rate of 62% and provides a clear indication that this flag is wholly unsuitable.

Example 1: Correct outlier flag.

```{r eg1, cache = TRUE}
extreme_date = as_date("1893-06-09")
stn_id = "ASN00040059"
stn_meta = meta_data |> 
  filter(id == stn_id)
fig_data_for_plot <- get_data_for_fig(extreme_date, stn_lat = stn_meta$latitude, stn_long = stn_meta$longitude, meta_data)
```

\

### R Solution

-   Don't filter the data by the "O" flag

### Proposed Solution

-   ??? if there is good coverage use spatial consistency checking / nearest neighbours ???
-   meta data or historical newspapers
-   Need a better solution.

### Fit a GEV with and without obs to show impact

-   Miralles and Davision trigger event paper
-   Venezuala example (classic)
-   Don't have other examples in the data
-   How many "O"'s aren't really "O"'s - false positive rate
-   Demonstrate this in the data somehow

### Untagged Accumulations

### Flagging systematic missingness

### Conclusions

Write some stuff here

### Appendix

Other fun quality issues:

-   Elevation

-   Records prior to European arrival in Australia

Curious but aside - How does this carry through to the gridded data.

### Example Issue Write Up

-   Explain the problem
-   Identify and show examples of this quality control issue in the data
-   Provide R function(s) to address the issue
-   Provide suggested updated quality flag
-   Depending on the issue, demonstrate the impact of this on a basic GEV analysis with and without the fix.
